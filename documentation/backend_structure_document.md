# Backend Structure Document: ai-customer-chat-rag

This document describes how the backend of the AI Customer Chat RAG application is built, hosted, and maintained. It’s written in everyday language so anyone can understand the setup without deep technical knowledge.

## 1. Backend Architecture

Overall, the backend is a simple Node.js server using Express.js and TypeScript. It handles basic health checks and (optionally) can proxy requests to Google’s Gemini API. The main design points are:

• Monolith service: All backend routes live in a single Express.js application.  
• Service Layer: Although the current front end talks directly to Gemini, the backend could house a service layer for future needs (e.g., user management, document processing).  
• TypeScript: Gives us type safety, so we catch mistakes early and improve maintainability.  
• Separation of concerns: Business logic (talking to Gemini or a database) is separated from route definitions.  

How it supports key goals:
• Scalability: Deploying the Node.js app on a cloud platform that supports automatic scaling (e.g., Google Cloud Run) allows the service to handle more traffic as needed.  
• Maintainability: Clear folder structure (`src/routes`, `src/services`) keeps code organized and easy to update.  
• Performance: The backend is lightweight—mostly passing requests through—so response times stay low.

## 2. Database Management

Right now, the application does not use a traditional database. Instead, it relies on Google’s Gemini API to store and index uploaded documents (the RAG store). All document text, embeddings, and citation metadata live in Gemini’s managed index.

Key points:
• No SQL/NoSQL database is configured in the repo.  
• Document storage and indexing are handled by Gemini’s internal systems.  
• The frontend holds temporary state (chat history, upload progress) in memory.

Note: In a future version, we could introduce a database (SQL or NoSQL) to store user sessions, persistent chat histories, or document metadata outside of Gemini.

## 3. Database Schema

Since there is no dedicated database, we don’t have an SQL schema. Instead, the Gemini RAG store uses its own internal schema to manage documents. In human-friendly terms, each RAG store contains:

• Document Record:
  – Document ID (unique)  
  – Filename and type (PDF, TXT, Markdown)  
  – Chunked text segments (to improve search accuracy)  
• Embedding Record:
  – Chunk ID  
  – Vector embedding (numeric array)  
  – Reference back to Document Record  
• Metadata:
  – Upload timestamp  
  – Source citation pointers (page number, line range)

If we did use PostgreSQL in the future, a simple schema might look like this (example only):

CREATE TABLE documents (
  id UUID PRIMARY KEY,
  filename TEXT NOT NULL,
  file_type TEXT NOT NULL,
  uploaded_at TIMESTAMP NOT NULL DEFAULT NOW()
);

CREATE TABLE chunks (
  id SERIAL PRIMARY KEY,
  document_id UUID REFERENCES documents(id),
  content TEXT NOT NULL,
  vector_embedding FLOAT8[]  
);

CREATE TABLE chat_history (
  id SERIAL PRIMARY KEY,
  session_id TEXT NOT NULL,
  user_message TEXT,
  ai_response TEXT,
  timestamp TIMESTAMP NOT NULL DEFAULT NOW()
);

## 4. API Design and Endpoints

Right now, the backend exposes only one public route, with room to grow:

• GET /health
  – Purpose: Simple check to confirm the server is running (returns 200 OK).  

In the current setup, the frontend’s `geminiService` directly calls Google’s Gemini API for:
• Uploading documents and creating a RAG store  
• Sending user queries and receiving answers with citations  
• Generating example questions

Future backend endpoints could include:
• POST /api/upload
  – Accepts file uploads and forwards them to Gemini  
• POST /api/query
  – Receives a user question, calls Gemini, and returns the answer  
• GET /api/examples
  – Fetches sample questions generated by Gemini  
• GET /api/stores
  – Lists RAG stores and associated documents

## 5. Hosting Solutions

To align with the Google Gemini API, the recommended hosting environment is Google Cloud Platform (GCP):

• Google Cloud Run (serverless containers)
  – Runs the Express.js backend  
  – Automatically scales up and down based on traffic  
  – Billing by the second, so cost-effective for variable loads  
• Firebase Hosting or Cloud Storage + CDN
  – Serves the static React frontend quickly and globally  
  – Built-in SSL, zero-config HTTPS, and content caching  

Benefits:
• Reliability: GCP SLAs at 99.95%+ uptime  
• Scalability: Automatic horizontal scaling handles peaks  
• Cost-effectiveness: Pay only for what you use, with free tiers available  

Alternative providers (AWS Lambda + S3/CloudFront, Azure Functions + Static Web Apps) are also viable.

## 6. Infrastructure Components

Here’s how different pieces fit together to give users a fast, reliable experience:

• Load Balancer (e.g., GCP HTTP(S) Load Balancer)
  – Distributes requests across multiple Cloud Run instances  
• CDN (Firebase CDN or Cloud CDN)
  – Caches static files (JavaScript, CSS, images) at edge locations  
  – Delivers content with low latency worldwide  
• Caching Layer (optional)
  – A Redis store (e.g., GCP Memorystore) can cache frequent API responses or embeddings  
• Container Registry / Artifact Storage
  – Stores Docker images for Cloud Run deployments  
• CI/CD Pipeline (e.g., Cloud Build, GitHub Actions)
  – Automates build, test, and deploy steps on each commit  

Together, these components keep response times low, ensure high availability, and streamline updates.

## 7. Security Measures

Protecting user data and API keys is a top priority. The following controls are in place or recommended:

• HTTPS Everywhere: All client-to-server and server-to-Gemini communications use TLS/SSL.  
• Secret Management: API keys and secrets live in GCP Secret Manager (or environment variables in Cloud Run), never in source code.  
• CORS Policy: Only trusted origins (your frontend URL) can call the backend.  
• Rate Limiting (future): Throttle incoming requests to prevent abuse (e.g., 100 requests/minute).  
• Input Validation: Sanitize and validate file uploads and user queries to guard against injection attacks.  
• Helmet Middleware: Sets HTTP headers to protect against common web vulnerabilities.  
• Audit Logging: Record admin-level actions and errors for forensic analysis.

Compliance with regulations (GDPR, CCPA) can be met by:
• Allowing users to delete their data  
• Encrypting data at rest (managed by GCP)  
• Providing clear privacy policies

## 8. Monitoring and Maintenance

Keeping an eye on performance and health is essential. We use:

• GCP Operations (formerly Stackdriver)
  – Logging: Collects logs from Cloud Run and backend services  
  – Metrics: Tracks CPU, memory, request latency, error rates  
  – Alerts: Sends notifications (email, Slack) if key metrics cross thresholds  
• Health Checks: Regular pings to `/health` endpoint  
• Error Reporting: Automatic crash reporting (uncaught exceptions, promise rejections)  
• Scheduled Maintenance:
  – Dependency updates (monthly or quarterly)  
  – Security patching for Docker images and base OS  
  – Backup/restore drills for future database if added

## 9. Conclusion and Overall Backend Summary

In summary, the ai-customer-chat-rag backend is currently a lightweight Express.js server in TypeScript that handles basic health checks and—if extended—can proxy AI requests to Google’s Gemini API. Document storage, indexing, and retrieval are all managed by Gemini’s internal RAG infrastructure, so there’s no dedicated database in the current version.

Key strengths:
• Fast setup and straightforward code maintenance  
• Seamless integration with Google Gemini for powerful RAG capabilities  
• Clear path to add databases, authentication, and more complex services  

Unique aspects:
• Leverages a monorepo approach with a distinct front end and back end but shares type definitions and tools  
• Offloads heavy text indexing and vector storage to a managed AI service  
• Can scale easily on GCP with minimal ops overhead

This setup meets the project goals of providing an AI-powered, document-based chat interface while keeping the backend simple, reliable, and ready for future growth.